import torch
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import tqdm
import numpy as np
import random
import time
from concurrent.futures import ThreadPoolExecutor

from utils import paths, read_pickle

import pdb


class ImageTextDataset(Dataset):
    """
    PyTorch requirements for custom Dataset:
    - Inheritance from torch.utils.data.Dataset
    - Implementations of:
        - __len__(self) --> int
        - __getitem__(self, idx) --> sample
    - Everything else is up to you!
    """

    def __init__(
            self, 
            index_imgs_class_enc, 
            index_imgs_rfpaths, 
            index_imgs_sids,
            text_preps,
            img_pp, 
            cached_imgs,
            num_workers,
        ):
        
        self.index_imgs_class_enc = index_imgs_class_enc
        self.index_imgs_rfpaths   = index_imgs_rfpaths
        self.index_imgs_sids      = index_imgs_sids
        self.text_preps           = text_preps
        self.img_pp               = img_pp
        self.cached_imgs          = cached_imgs

        self.n_samples = len(self.index_imgs_class_enc)

        if self.cached_imgs in ("pl", "pp"):
            time_start = time.time()

            def load_pp_img(rfpath):
                img   = Image.open(paths["nymph"] / "images" / rfpath).convert("RGB")
                return img if cached_imgs == "pl" else img_pp(img)

            # load all images into memory (pl: as PIL images; pp: as preprocessed tensors)
            self.imgs_mem = []
            with ThreadPoolExecutor(max_workers=num_workers) as exe:
                for img in tqdm.tqdm(exe.map(load_pp_img, self.index_imgs_rfpaths),
                                     total=len(self.index_imgs_rfpaths),
                                     desc ="Caching Images"):
                    self.imgs_mem.append(img)

            time_end     = time.time()
            time_elapsed = time_end - time_start
            print(f"Time Elapsed (preload): {time_elapsed:.1f} s")

    def __len__(self):
        return self.n_samples
    
    # gets called in the background on indices of batch N+1 while GPU (and main process) are busy running img2txt_classify() on batch N
    def __getitem__(self, idx):
        """
        Returns transformed image and class encoding.
        idx --> sample (preprocessed image, class encoding)
        """
        class_enc = self.index_imgs_class_enc[idx]
        sid       = self.index_imgs_sids[idx]

        text = gen_text(sid, self.text_preps)

        if self.cached_imgs == "pp":
            img_t = self.imgs_mem[idx]
        elif self.cached_imgs == "pl":
            img   = self.imgs_mem[idx]
            img_t = self.img_pp(img)
        else:
            # load + preprocess image
            img   = Image.open(paths["nymph"] / "images" / self.index_imgs_rfpaths[idx]).convert("RGB")
            img_t = self.img_pp(img)
        
        return img_t, class_enc, text

def collate_fn(batch):
    """
    collate_fn takes list of individual samples from Dataset and merges them into a single batch
    augmentation can be done here methinks
    """
    imgs_b, class_encs_b, texts_b = zip(*batch)

    imgs_b = torch.stack(imgs_b, dim=0)  # --- Tensor(B, C, H, W)

    return imgs_b, class_encs_b, list(texts_b)

def spawn_indexes_imgs(split_type, split_name):
    """

    Args:
    - split_type --- [str] --- "train" / "id_val" / "id_test" / "ood_val" / "ood_test"
    - split_name --- [str] --- Name of the split directory e.g. "A" / "B" / etc.
    """
    data_index = read_pickle(paths["metadata_o"] / f"data_indexes/{split_name}/{split_type}.pkl")

    index_imgs_rfpaths = data_index["rfpaths"]
    index_imgs_sids    = data_index["sids"]

    sid_2_class_enc      = {}
    class_enc_new        = 0
    index_imgs_class_enc = []
    for sid in index_imgs_sids:
        if sid not in sid_2_class_enc.keys():
            sid_2_class_enc[sid] = class_enc_new
            class_enc_new += 1
        index_imgs_class_enc.append(sid_2_class_enc[sid])

    return np.array(index_imgs_class_enc), np.array(index_imgs_rfpaths), index_imgs_sids, sid_2_class_enc

def spawn_indexes_txts(sid_2_class_enc, text_preps):
    """
    Think this is still needed but only for eval

    Note: This was split apart from the spawn_indexes_imgs() logic for ease of setting up the mixed text-types experiment
    
    Args:
    - sid_2_class_enc --- [dict(sid --> class enc)] --- generated by spawn_indexes_imgs()
    """

    index_txts_sids      = list(sid_2_class_enc.keys())
    index_txts_class_enc = np.array(list(sid_2_class_enc.values()))

    index_txts = [gen_text(sid, text_preps) for sid in index_txts_sids]

    return index_txts, index_txts_class_enc

def spawn_dataloader(
        index_imgs_class_enc,
        index_imgs_rfpaths,
        index_imgs_sids,
        text_preps,
        batch_size,
        shuffle,
        drop_last,
        img_pp,
        cached_imgs,
        num_workers,
        prefetch_factor,
    ):
    """

    Args:
    - index_imgs_class_enc --- [list(int)] --------- Class encodings (data index)
    - index_imgs_rfpaths ----- [list(int)] --------- Relative filepaths to images (data index)
    - index_imgs_sids -------- [list(str)] --------- Species Identifiers (data index)
    - text_preps ------------- [list(list(str))] --- List of text prepending selections that are randomly picked from to assemble train texts
    - batch_size ------------- [int] --------------- Batch size
    - shuffle ---------------- [bool] -------------- Whether to shuffle samples between cycles
    - drop_last -------------- [bool] -------------- Whether to drop partial batch at the end of epoch (only need this arg for train)
    - img_pp ----------------- [callable] ---------- The image preprocessor
    - cached_imgs ------------ [bool] -------------- Whether to cache images in memory
    - num_workers ------------ [int] --------------- Parallelism
    - prefetch_factor -------- [int] --------------- How many batches each worker will load in advance;
                                                     Higher prefetch_factor increases throughput, higher RAM cost;
                                                     Only takes effect when num_workers > 0
    """

    dataset = ImageTextDataset(
        index_imgs_class_enc,
        index_imgs_rfpaths,
        index_imgs_sids,
        text_preps,
        img_pp,
        cached_imgs,
        num_workers,
    )

    loader = DataLoader(
        dataset,
        batch_size        =batch_size,
        shuffle           =shuffle,
        num_workers       =num_workers,
        pin_memory        =True,  # (True) speeds up host --> GPU copies, higher RAM cost
        prefetch_factor   =prefetch_factor,
        collate_fn        =collate_fn,
        drop_last         =drop_last,
        persistent_workers=True,
    )

    return loader

def gen_text(sid, text_preps):

    genus, species_epithet = sid.split("_", 1)
    text = f"{genus} {species_epithet}"

    for text_preps_cat in reversed(text_preps):  # iterate through text prependending categories in reversed order
        text = random.choice(text_preps_cat) + text  # select random prepending from prepending-category, prepend to text

    return text
