# study_name:      dev
study_name:      class_imb_sig
# experiment_name: dev
experiment_name: mp_posneg_if-1-0_cp2
seed:            42
split_name:      S29-42
# split_name:      dev

verbose_batch_loss: false

# whether to allow trials to overwrite data at the study/experiment/trial level (for dev/debug, should otherwise be set to false)
allow_overwrite_trial: true
allow_diff_study:      true
allow_diff_experiment: true

model_type: siglip_vitb16  # common options: {bioclip, clip_vitb16, siglip_vitb16}
non_causal: false  # (true) converts text encoder causal attention mask to non-causal (zeros on diagonal + lower triangle, -inf on upper --> all zeros) -- only applicable to CLIP and ViTamin (SigLIP already uses non-causal i.e. bidirectional)

loss_type: sigmoid  # options: {infonce1, infonce2, sigmoid, mse, huber}
targ_type: multipos  # options: {pairwise, multipos, hierarchical}
regression:
  temp:       true  # analog to CLIP + SigLIP temperature (scales logits)
  bias:       true  # analog to SigLIP bias (shifts logits)
  scale_type: 1  # options: {1, 2}; computation type used for scaling logits to range [0, 1]; type 1 uses an affine tranform, preceded by a tanh if temp and/or bias are used, type 2 uses a sigmoid
  huber_beta: 0.1
class_weighting:
  type:     inv_freq  # options: {null, inv_freq, class_balanced}
  if_gamma: 1.0  # (0.0, 1.0]; "inverse-frequency gamma", only used for type: inv_freq; 1 / n_c^(if_gamma); if_gamma = 1.0 --> 1 / n_c (classic scheme), if_gamma = 0.5 --> 1 / sqrt(n_c), if_gamma = 0.0 --> no weighting (uniform)
  cb_beta:  0.9999  # [0.0, 1.0); only used for type: class_balanced; controls aggressiveness
  cp_type:  2  # options: {1, 2}; class-pair computation type; 1: vanilla (i2t, t2i for same pair treated as separate "classes"), 2: negative pairs multiplied by 2, upper triangle extracted and used to normalize, full matrix constructed with normalized weights (i2t, t2i for same pair treated as same "classes")
focal:
  enabled:   false
  gamma:     2.0  # [0, inf); down-weights easy samples, larger gamma --> stronger focus on hard cases; 1-2 for softmax (InfoNCE), 2 for sigmoid; FL reduces to CE when gamma = 0
  comp_type: 2  # options: {1, 2}; continuous-form focal loss computation type; identical behavior to discrete form when targets=0/1
alpha_pos: 0.5  # [0, 1]; only relevant for sigmoid (and maybe regression); 0.5 equivalent to no pos/neg weighting; alpha_neg = 1 - alpha_pos
# alpha_pos: 0.9995117188  # (pairwise ratio for batch size = 2048) --- can get rid of this if it's not compatible with hierarchical
dyn_posneg: true  # dynamic reweighting of pos/neg  overrides alpha_pos; intended for use with multi-pos, also convenient for pairwise (verify equivalence), not compatible with hierarchical

n_epochs:    200
chkpt_every: 1
# batch_size:  32
# batch_size:  512
# batch_size:  1024
batch_size:  2048
# batch_size:  4096
# batch_size:  8192
# batch_size:  16384
# batch_size:  32768

drop_partial_batch_train: true

lr_init:       1.0e-5
lr_sched_type: exp
weight_decay:  0.0  # CLIP paper recommends weight_decay = 0.2 <-- this may be just for pretrain, look into it
beta1:         0.9  # CLIP paper recommends Beta1 = 0.9
beta2:         0.95  # seminal papers recommend Beta2 = 0.999 (CLIP ResNet) / 0.98 (CLIP ViT) / 0.95 (SigLIP) ~ careful, may be for pretraining not fine-tuning
eps:           1.0e-6  # CLIP paper recommends eps = 1.0e-8 (ResNet) / 1.0e-6 (ViT)

freeze_text_encoder:  false
freeze_image_encoder: false

cached_imgs: null  # options: {null, pl, pp}
mixed_prec:  true  # mixed precision for training and validation
act_chkpt:   true  # activation checkpointing

# text_preps_type_train: mixed
text_preps_type_train: combo_temps
text_preps_type_val:   bioclip_sci
