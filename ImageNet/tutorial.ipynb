{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc5e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83e630c4",
   "metadata": {},
   "source": [
    "This tutorial assumes a general knowledge of Deep Learning (e.g. CS 7643)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae52de0",
   "metadata": {},
   "source": [
    "Setup:\n",
    "1. Pull repo via `git clone xyz`\n",
    "2. etc\n",
    "3. Install env\n",
    "4. etc\n",
    "5. etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189ef014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn.functional as F\n",
    "import open_clip\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from feat_eng import IMAGENET_CLASSES, IMAGENET_TEMPLATES\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e69c2c6",
   "metadata": {},
   "source": [
    "Recommended resources:\n",
    "- GPU: x\n",
    "- RAM: x\n",
    "- etc.\n",
    "\n",
    "Note: The environment is not configured to support newer GPUs e.g. (H100/H200/B200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd2e63",
   "metadata": {},
   "source": [
    "Create and activate environment with:<br>\n",
    "`conda env create -f environment.yaml`<br>\n",
    "`conda activate imagenet-zeroshot`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52652844",
   "metadata": {},
   "source": [
    "Want to move everything not important to the learning objectives of the tutorial outta here\n",
    "\n",
    "Dimension annotations:<br>\n",
    "B: Batch dimension i.e. sample dimension<br>\n",
    "D: Embedding dimension<br>\n",
    "T: Context length (77 for CLIP)<br>\n",
    "L: Num. classes (1000 in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25031f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2_048\n",
    "# BATCH_SIZE = 8\n",
    "\n",
    "NUM_WORKERS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b8681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dpath_imagenet = Path(\"/data/ai/ref-data/image/ImageNet/imagenet1k\")\n",
    "dpath_valid    = dpath_imagenet / \"Data/CLS-LOC/val\"\n",
    "fpath_csv_val  = dpath_imagenet / \"LOC_val_solution.csv\"\n",
    "fpath_synmap   = dpath_imagenet / \"LOC_synset_mapping.txt\"\n",
    "\n",
    "root_ilsvrc = dpath_imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f955b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def build_class_protos_text_feat_eng(model, tokenizer, txts_class: List[str], txt_templates: List[str], device: torch.device) -> torch.Tensor:\n",
    "\n",
    "    protos = []\n",
    "\n",
    "    for txt in tqdm(txts_class):\n",
    "\n",
    "        txts      = [temp.format(txt) for temp in txt_templates]\n",
    "        toks_txts = tokenizer(txts).to(device)\n",
    "\n",
    "        embs_txts = model.encode_text(toks_txts)\n",
    "        embs_txts = embs_txts / embs_txts.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        proto = embs_txts.mean(dim=0)\n",
    "        proto = proto / proto.norm()\n",
    "        protos.append(proto)\n",
    "\n",
    "    protos = torch.stack(protos, dim=1)\n",
    "        \n",
    "    return protos\n",
    "\n",
    "# METRICS\n",
    "\n",
    "@torch.no_grad()\n",
    "def topk_accuracy(logits: torch.Tensor, targs: torch.Tensor, ks=(1, 5)) -> List[float]:\n",
    "    \"\"\"\n",
    "    Produces a list of Precision@k batch-average scores given logits and targets.\n",
    "    Prec@k's included are specified with the `ks` arg, configured to Prec@1 and Prec@5 by default.\n",
    "    \"\"\"\n",
    "    B       = targs.size(0)\n",
    "    k_max   = max(ks)\n",
    "    _, pred = logits.topk(k_max, dim=1)\n",
    "\n",
    "    pred    = pred.t()\n",
    "    correct = pred.eq(targs.view(1, -1))\n",
    "\n",
    "    return [correct[:k].reshape(-1).float().sum() / B for k in ks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6b04cb",
   "metadata": {},
   "source": [
    "# Initialize ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74af0da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_wts = torchvision.models.ResNet50_Weights.IMAGENET1K_V1\n",
    "resnet_tf  = resnet_wts.transforms()\n",
    "resnet     = torchvision.models.resnet50(weights=resnet_wts).to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023f94a8",
   "metadata": {},
   "source": [
    "# Initialize Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e342623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these can probably be added to the inference function\n",
    "\n",
    "dataset_resnet    = ImageFolder(dpath_valid, transform=resnet_tf)\n",
    "dataloader_resnet = DataLoader(dataset_resnet, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19082a7",
   "metadata": {},
   "source": [
    "The code is arranged in such a way to highlight the similarities and differences between classic and VLM paradigms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db5014f",
   "metadata": {},
   "source": [
    "# ResNet-50 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n         = 0\n",
    "prec1_sum = 0.0\n",
    "prec5_sum = 0.0\n",
    "with torch.no_grad():\n",
    "    for imgs_b, targs_enc_b in tqdm(dataloader_resnet):\n",
    "        \"\"\"\n",
    "        imgs_b -------- Tensor(B, 3, 224, 224)\n",
    "        targs_enc_b --- Tensor(B)\n",
    "        \"\"\"\n",
    "        B           = targs_enc_b.size(0)\n",
    "        imgs_b      = imgs_b.to(device, non_blocking=True)\n",
    "        targs_enc_b = targs_enc_b.to(device, non_blocking=True)\n",
    "\n",
    "        logits = resnet(imgs_b)\n",
    "\n",
    "        prec1, prec5 = topk_accuracy(logits, targs_enc_b)\n",
    "        prec1_sum += prec1.item() * B\n",
    "        prec5_sum += prec5.item() * B\n",
    "        n += B\n",
    "print(\n",
    "    f\"ResNet-50:\",\n",
    "    f\"Prec@1 --- {prec1_sum / n:.4f}\",\n",
    "    f\"Prec@5 --- {prec5_sum / n:.4f}\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa03b7d",
   "metadata": {},
   "source": [
    "# VLM Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905198e7",
   "metadata": {},
   "source": [
    "(1) logits are scaled with a temperature parameter during training, which adjusts the sharpness of xyz, but for inference it doesn't matter because applying the scaling doesn't change logit ranking (i.e. doesn't change ordering if logits were to be sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def run_inference_vlm(model, dataloader, class_protos_txt, device, model_name):\n",
    "    n_samps   = 0\n",
    "    prec1_sum = 0.0\n",
    "    prec5_sum = 0.0\n",
    "    for imgs_b, targs_enc_b in tqdm(dataloader):\n",
    "        \"\"\"\n",
    "        imgs_b -------- Tensor(B, 3, 224, 224)\n",
    "        targs_enc_b --- Tensor(B)\n",
    "        \"\"\"\n",
    "        B           = targs_enc_b.size(0)\n",
    "        imgs_b      = imgs_b.to(device, non_blocking=True)\n",
    "        targs_enc_b = targs_enc_b.to(device, non_blocking=True)\n",
    "\n",
    "        embs_img_b = model.encode_image(imgs_b)  # run batch of images through image encoder to produce embeddings, a single D-dimensional embedding produced for each image\n",
    "        embs_img_b = F.normalize(embs_img_b, dim=1)  # embeddings are normalized to unit length\n",
    "        logits     = embs_img_b @ class_protos_txt  # (1) <-- make this like a red circle with a white 1 or something\n",
    "\n",
    "        prec1, prec5 = topk_accuracy(logits, targs_enc_b)\n",
    "        prec1_sum += prec1.item() * B\n",
    "        prec5_sum += prec5.item() * B\n",
    "        n_samps += B\n",
    "    print(\n",
    "        f\"{model_name}:\",\n",
    "        f\"Prec@1 --- {prec1_sum / n_samps:.4f}\",\n",
    "        f\"Prec@5 --- {prec5_sum / n_samps:.4f}\",\n",
    "        sep=\"\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c069e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vlm_configs = [\n",
    "    (\"ViT-B-32\",            \"openai\", True),\n",
    "    (\"ViT-B-16\",            \"openai\", True),\n",
    "    (\"ViT-L-14\",            \"openai\", True),\n",
    "    (\"ViT-L-14-336\",        \"openai\", True),\n",
    "    (\"ViT-B-16-SigLIP\",     \"webli\",  False),\n",
    "    (\"ViT-B-16-SigLIP-256\", \"webli\",  False),\n",
    "    (\"ViT-L-16-SigLIP-256\", \"webli\",  False),\n",
    "]\n",
    "\n",
    "for id_model, pretrained, quick_gelu in vlm_configs:\n",
    "\n",
    "    model, _, model_tf = open_clip.create_model_and_transforms(id_model, pretrained=pretrained, quick_gelu=quick_gelu, device=device)\n",
    "    model.eval()\n",
    "    tokenizer = open_clip.get_tokenizer(id_model)\n",
    "\n",
    "    dataset    = ImageFolder(dpath_valid, transform=model_tf)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    class_protos_txt = build_class_protos_text_feat_eng(model, tokenizer, IMAGENET_CLASSES, IMAGENET_TEMPLATES, device).to(device)\n",
    "\n",
    "    run_inference_vlm(model, dataloader, class_protos_txt, device, id_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e5dcd",
   "metadata": {},
   "source": [
    "Note: `RN50` in the printout refers to CLIP ResNet-50 and `ResNet` refers to the standalone ResNet-50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc890fa",
   "metadata": {},
   "source": [
    "Maybe perform an ablation of the text ensemble: just encode the labels straight up and compare to the templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b67ae93",
   "metadata": {},
   "source": [
    "Not the 76.2% reported in the seminal CLIP paper (Table 11), but close enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf242e",
   "metadata": {},
   "source": [
    "oo also compare CLIP_MODEL = \"RN50\", PRETRAINED = \"openai\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2736042",
   "metadata": {},
   "source": [
    "We conclude with a discussion of the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95d2b4e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
