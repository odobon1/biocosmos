# study_name:      class_imb
study_name:      dev
# experiment_name: clip_inv-freq-0-5_focal_S29-42
experiment_name: dev
seed:            42
split_name:      S29-42
# split_name:      D2
# split_name:      dev

verbose_batch_loss: false

# whether to allow trials to overwrite data at the study/experiment/trial level (for dev/debug, should otherwise be set to false)
allow_overwrite_trial: true
allow_diff_study:      true
allow_diff_experiment: true

model_type: clip_vitb16  # common options: bioclip / clip_vitb16 / siglip_vitb16
loss_type:  infonce  # common options: infonce / pairwise_sigmoid / pairwise_sigmoid_upwtdpos / multipos_sigmoid
class_weighting:
  type:     inv_freq  # options: null / inv_freq / class_balanced
  if_gamma: 0.5  # (0.0, 1.0]; "inverse-frequency gamma", only used for type: inv_freq; 1 / n_c^(if_gamma); if_gamma = 1.0 --> 1 / n_c (classic scheme), if_gamma = 0.5 --> 1 / sqrt(n_c), if_gamma = 0.0 --> no weighting (uniform)
  cb_beta:  0.9999  # [0.0, 1.0); only used for type: class_balanced; controls aggressiveness
  cb_eps:   1e-8  # minimum on unnormalized weights for numerical stability; only applicable for type: class_balanced
focal:
  enabled:   true
  gamma:     2.0  # [0, inf); down-weights easy samples, larger gamma --> stronger focus on hard cases; 1-2 for softmax (InfoNCE), 2 for sigmoid; FL reduces to CE when gamma = 0
  alpha_pos: 0.5  # how much weight positives get relative to negatives inside the focal loss term e.g. alpha_t = alpha_pos for positives, alpha_t = (1 - alpha_pos) for negatives; only relevant for sigmoid-based losses
  comp_type: 1  # computation type; options: 1 / 2 / 3; 1: discrete targets only (positive/negative), 2/3: proposed continuous alternatives in support of hierarchical loss; nearly identical behavior for all 3 variants when targets=0/1; only relevant for sigmoid-based losses

n_epochs:         10
checkpoint_every: 1
# batch_size_train: 32
# batch_size_train: 512
batch_size_train: 1024
# batch_size_train: 2048
# batch_size_train: 4096
# batch_size_train: 8192
# batch_size_train: 16384
# batch_size_train: 32768
batch_size_val:   1024

drop_partial_batch_train: true

weight_decay: 0.0  # CLIP paper recommends weight_decay = 0.2
beta1:        0.9  # CLIP paper recommends Beta1 = 0.9
beta2:        0.98  # CLIP paper recommends Beta2 = 0.999 (ResNet) / 0.98 (ViT)
eps:          1.0e-6  # CLIP paper recommends eps = 1.0e-8 (ResNet) / 1.0e-6 (ViT)

lr_init: 1.0e-5
lr_sched:
  # Exponential LR Scheduler
  type: exp
  args:
    lr_min: 1.0e-9
  args_sched:
    gamma: 0.7  # decay rate
  # # Plateau LR Scheduler
  # type: plat  # may also want to tweak threshold / eps
  # args:
  #   reset_best_val: true  # (true) reset best validation score after LR drop to avoid early plateauing
  #   val_type:       loss  # options: loss / perf
  # args_sched:
  #   factor:   0.61803398875  # golden ratio
  #   patience: 1
  #   cooldown: 1
  #   min_lr:   1.0e-7
  # # Cosine LR Scheduler
  # type: cos
  # args:
  #   dummy: null
  # args_sched:
  #   T_max:   5  # half-period
  #   eta_min: 1.0e-7
  # # Cosine Warm-Restarts LR Scheduler
  # type: coswr
  # args:
  #   dummy: null
  # args_sched:
  #   T_0:     7  # period
  #   eta_min: 1.0e-7
  # # Cosine X Exponential LR Scheduler
  # type: cosXexp
  # args:
  #   gamma:      0.98
  #   period:     10
  #   peak_ratio: 10
  #   lr_nom_min: 5.0e-7
  # Cosine Warm-Restarts X Exponential LR Scheduler
  # type: coswrXexp
  # args:
  #   gamma:      0.98
  #   period:     10
  #   peak_ratio: 10
  #   lr_nom_min: 5.0e-7

freeze_text_encoder:  false
freeze_image_encoder: false

cached_imgs: null  # options: null / pl / pp
mixed_prec:  true  # mixed precision for training and validation
act_chkpt:   false  # activation checkpointing

text_preps_type_train: mixed
text_preps_type_val:   bioclip_sci
