study_name:      dev
# study_name:      hierarchical100
experiment_name: dev
# experiment_name: infonce2_h_NoT
seed:            42
split_name:      S29-42
# split_name:      dev

verbose_batch_loss: false

# whether to allow trials to overwrite data at the study/experiment/trial level (for dev/debug, should otherwise be set to false)
allow_overwrite_trial: true
allow_diff_study:      true
allow_diff_experiment: true

model_type: clip_vitb16  # common options: {bioclip, clip_vitb16, siglip_vitb16}
loss_type:  huber  # options: {infonce1, infonce2, sigmoid, mse, huber}
targ_type:  hierarchical  # options: {pairwise, multipos, hierarchical}
regression:
  temp:  true  # analog to CLIP + SigLIP temperature (scales logits)
  bias:  true  # analog to SigLIP bias (shifts logits)
  scale: 1  # options: {1, 2}; computation type used for scaling logits to range [0, 1]; type 1 uses an affine tranform, preceded by a tanh if temp and/or bias are used, type 2 uses a sigmoid
  huber_beta: 0.1
class_weighting:
  type:     inv_freq  # options: {null, inv_freq, class_balanced}
  if_gamma: 0.5  # (0.0, 1.0]; "inverse-frequency gamma", only used for type: inv_freq; 1 / n_c^(if_gamma); if_gamma = 1.0 --> 1 / n_c (classic scheme), if_gamma = 0.5 --> 1 / sqrt(n_c), if_gamma = 0.0 --> no weighting (uniform)
  cb_beta:  0.9999  # [0.0, 1.0); only used for type: class_balanced; controls aggressiveness
  cp_type:  2  # options: {1, 2}; class-pair computation type; 1: vanilla (i2t, t2i for same pair treated as separate "classes"), 2: negative pairs multiplied by 2, upper triangle extracted and used to normalize, full matrix constructed with normalized weights (i2t, t2i for same pair treated as same "classes")
focal:
  enabled:   true
  gamma:     2.0  # [0, inf); down-weights easy samples, larger gamma --> stronger focus on hard cases; 1-2 for softmax (InfoNCE), 2 for sigmoid; FL reduces to CE when gamma = 0
  comp_type: 2  # options: {1, 2}; continuous-form focal loss computation type; identical behavior to discrete form when targets=0/1
alpha_pos: 0.5  # [0, 1]; only relevant for sigmoid (and maybe regression); 0.5 equivalent to no pos/neg weighting; alpha_neg = 1 - alpha_pos

n_epochs:    10
chkpt_every: 1
# batch_size:  32
# batch_size:  512
# batch_size:  1024
# batch_size:  2048
batch_size:  4096
# batch_size:  8192
# batch_size:  16384
# batch_size:  32768

drop_partial_batch_train: true

lr_init:       1.0e-5
lr_sched_type: exp
weight_decay:  0.0  # CLIP paper recommends weight_decay = 0.2 <-- this may be just for pretrain, look into it
beta1:         0.9  # CLIP paper recommends Beta1 = 0.9
beta2:         0.98  # seminal papers recommend Beta2 = 0.999 (CLIP ResNet) / 0.98 (CLIP ViT) / 0.95 (SigLIP) ~ careful, may be for pretraining not fine-tuning
eps:           1.0e-6  # CLIP paper recommends eps = 1.0e-8 (ResNet) / 1.0e-6 (ViT)

freeze_text_encoder:  false
freeze_image_encoder: false

cached_imgs: null  # options: {null, pl, pp}
mixed_prec:  true  # mixed precision for training and validation
act_chkpt:   true  # activation checkpointing

# text_preps_type_train: mixed
text_preps_type_train: combo_temps
text_preps_type_val:   bioclip_sci
