{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85e64ea0",
   "metadata": {},
   "source": [
    "# From Pixels to Prompts:\n",
    "## A Crash Course on Zero-Shot Classification Using Language-Vision Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e630c4",
   "metadata": {},
   "source": [
    "This tutorial assumes a general knowledge of deep learning and familiarity with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae52de0",
   "metadata": {},
   "source": [
    "Setup:\n",
    "1. Pull repo via `git clone xyz`\n",
    "2. etc\n",
    "3. Install env\n",
    "4. etc\n",
    "5. etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e69c2c6",
   "metadata": {},
   "source": [
    "Recommended resources:\n",
    "- GPU: x\n",
    "- RAM: x\n",
    "- etc.\n",
    "\n",
    "Note: The environment does not support the newer Hopper nor Blackwell GPU models (H100/H200/B100/B200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd2e63",
   "metadata": {},
   "source": [
    "Create and activate environment with:<br>\n",
    "`conda env create -f environment.yaml`<br>\n",
    "`conda activate imagenet-zeroshot`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783bb446",
   "metadata": {},
   "source": [
    "(Note that everything below this point is very similar to what's in the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83d25fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/arthur.porto-biocosmos/odobon3.gatech/.conda/envs/biocosmos_o_b200/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import open_clip\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from feat_eng import IMAGENET_CLASSES, IMAGENET_TEMPLATES\n",
    "from utils import spawn_dataloader, batch_prec1\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52652844",
   "metadata": {},
   "source": [
    "Want to move everything not important to the learning objectives of the tutorial outta here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84b8681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 32\n",
    "# BATCH_SIZE = 512\n",
    "# BATCH_SIZE = 1024\n",
    "BATCH_SIZE = 2048\n",
    "# BATCH_SIZE = 4096  # resnet: 54.6, \n",
    "# BATCH_SIZE = 8192\n",
    "# BATCH_SIZE = 16384\n",
    "# BATCH_SIZE = 32768\n",
    "\n",
    "N_WORKERS  = 8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dpath_valid = Path(\"/data/ai/ref-data/image/ImageNet/imagenet1k/Data/CLS-LOC/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f955b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def build_class_protos_text(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    txts_class: List[str], \n",
    "    txt_templates: List[str], \n",
    "    device: torch.device) -> torch.Tensor:\n",
    "\n",
    "    protos = []\n",
    "\n",
    "    for txt in tqdm(txts_class, desc=\"Creating text embeddings\"):\n",
    "\n",
    "        txts      = [temp.format(txt) for temp in txt_templates]\n",
    "        toks_txts = tokenizer(txts).to(device)\n",
    "\n",
    "        embs_txts = model.encode_text(toks_txts)\n",
    "        embs_txts = embs_txts / embs_txts.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        proto = embs_txts.mean(dim=0)\n",
    "        proto = proto / proto.norm()\n",
    "        protos.append(proto)\n",
    "\n",
    "    protos = torch.stack(protos, dim=1)\n",
    "    \n",
    "    return protos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a52d145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_inference(model, dataloader, device, model_name, class_embs_txt=None):\n",
    "    # (1) Initialize some parameters for performance bookkeeping\n",
    "    n_samps   = 0\n",
    "    prec1_sum = 0.0\n",
    "    # (2) Iterate over validation data, loading up a batch of B images and corresponding targets at a time\n",
    "    for imgs_b, targs_enc_b in tqdm(dataloader, desc=\"Evaluation\"):\n",
    "        \"\"\"\n",
    "        DELETE\n",
    "        imgs_b -------- Tensor(B, 3, 224, 224)\n",
    "        targs_enc_b --- Tensor(B)\n",
    "        \"\"\"\n",
    "        # (3) Number of samples in the current batch\n",
    "        B           = targs_enc_b.size(0)\n",
    "        # (4) Send batched images and targets tensors to GPU\n",
    "        imgs_b      = imgs_b.to(device, non_blocking=True)\n",
    "        targs_enc_b = targs_enc_b.to(device, non_blocking=True)\n",
    "\n",
    "        # (5) Perform batch inference with ResNet-50 or VLM. This will be explained next.\n",
    "        if model_name == \"ResNet-50\":\n",
    "            logits = batch_inference_res(model, imgs_b)\n",
    "        else:\n",
    "            logits = batch_inference_vlm(model, imgs_b, class_embs_txt)\n",
    "\n",
    "        # (x) Compute Top-1 Precision (Prec@1) for batch, update Prec@1 sum and sample count\n",
    "        prec1 = batch_prec1(logits, targs_enc_b)\n",
    "        prec1_sum += prec1.item() * B\n",
    "        n_samps += B\n",
    "\n",
    "    print(\n",
    "        f\"\",\n",
    "        f\"Prec@1: {prec1_sum / n_samps:.1%}\",\n",
    "        f\"\",\n",
    "        sep=\"\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def batch_inference_res(model, imgs_b):\n",
    "\n",
    "    logits = model(imgs_b)\n",
    "\n",
    "    return logits\n",
    "\n",
    "@torch.no_grad()\n",
    "def batch_inference_vlm(model, imgs_b, class_embs_txt):\n",
    "\n",
    "    embs_img_b = model.encode_image(imgs_b)  # run batch of images through image encoder to produce embeddings, a D-dimensional embedding produced for each image\n",
    "    embs_img_b = F.normalize(embs_img_b, dim=1)  # embeddings are normalized to unit length\n",
    "    logits     = embs_img_b @ class_embs_txt  # (1) <-- make this like a red circle with a white 1 or something\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb04d87",
   "metadata": {},
   "source": [
    "# Initialize ResNet-50 + DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9fcac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_wts    = torchvision.models.ResNet50_Weights.IMAGENET1K_V1\n",
    "resnet_img_pp = resnet_wts.transforms()\n",
    "resnet        = torchvision.models.resnet50(weights=resnet_wts).to(device).eval()\n",
    "\n",
    "dataloader_resnet = spawn_dataloader(dpath_valid, resnet_img_pp, BATCH_SIZE, N_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55db834c",
   "metadata": {},
   "source": [
    "# ResNet-50 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac6a283b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet-50:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 25/25 [00:52<00:00,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prec@1: 76.2%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ResNet-50:\")\n",
    "run_inference(resnet, dataloader_resnet, device, \"ResNet-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e541f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90dff708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN INFERENCE OF SINGLE MODEL (CLIP FLAGSHIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa03b7d",
   "metadata": {},
   "source": [
    "# VLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98a840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9b05bdd",
   "metadata": {},
   "source": [
    "`open_clip.list_pretrained()` can be executed to view pretrained VLMs available through `open_clip`. Running this function displays architectures along with the dataset it was pretrained on which is needed to initialize the VLM image preprocessor. Unfortunately, this function does not also display the recommended `quick_gelu` setting so that is something the reader will have to look up on their own per model, but in general, the CLIP architectures performed pretraining using QuickGeLU and all the others did not. Typically, models initialized with pretrained weights from OpenAI should use QuickGeLU and all others not, as we will soon see.\n",
    "\n",
    "Interested readers can learn more about some of the more prominent open-source pretraining datasets at the following links:\n",
    "* [LAION](https://laion.ai/blog/laion-5b/)\n",
    "* [CommonPool](https://ar5iv.labs.arxiv.org/html/2304.14108)\n",
    "* [WebLI](https://research.google/blog/pali-scaling-language-image-learning-in-100-languages/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7780a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <img src=\"images/jl_launcher.png\">\n",
    "# <center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2e668b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RN50', 'openai'),\n",
       " ('RN50', 'yfcc15m'),\n",
       " ('RN50', 'cc12m'),\n",
       " ('RN101', 'openai'),\n",
       " ('RN101', 'yfcc15m'),\n",
       " ('RN50x4', 'openai'),\n",
       " ('RN50x16', 'openai'),\n",
       " ('RN50x64', 'openai'),\n",
       " ('ViT-B-32', 'openai'),\n",
       " ('ViT-B-32', 'laion400m_e31'),\n",
       " ('ViT-B-32', 'laion400m_e32'),\n",
       " ('ViT-B-32', 'laion2b_e16'),\n",
       " ('ViT-B-32', 'laion2b_s34b_b79k'),\n",
       " ('ViT-B-32', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-B-32', 'datacomp_m_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_clip_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_laion_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_image_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_text_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_basic_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_s128m_b4k'),\n",
       " ('ViT-B-32', 'datacomp_s_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_clip_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_laion_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_image_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_text_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_basic_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_s13m_b4k'),\n",
       " ('ViT-B-32', 'metaclip_400m'),\n",
       " ('ViT-B-32', 'metaclip_fullcc'),\n",
       " ('ViT-B-32-256', 'datacomp_s34b_b86k'),\n",
       " ('ViT-B-16', 'openai'),\n",
       " ('ViT-B-16', 'laion400m_e31'),\n",
       " ('ViT-B-16', 'laion400m_e32'),\n",
       " ('ViT-B-16', 'laion2b_s34b_b88k'),\n",
       " ('ViT-B-16', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-B-16', 'datacomp_l_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_clip_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_laion_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_image_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_text_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_basic_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_s1b_b8k'),\n",
       " ('ViT-B-16', 'dfn2b'),\n",
       " ('ViT-B-16', 'metaclip_400m'),\n",
       " ('ViT-B-16', 'metaclip_fullcc'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e31'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'openai'),\n",
       " ('ViT-L-14', 'laion400m_e31'),\n",
       " ('ViT-L-14', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'laion2b_s32b_b82k'),\n",
       " ('ViT-L-14', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_clip_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_laion_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_s13b_b90k'),\n",
       " ('ViT-L-14', 'metaclip_400m'),\n",
       " ('ViT-L-14', 'metaclip_fullcc'),\n",
       " ('ViT-L-14', 'dfn2b'),\n",
       " ('ViT-L-14', 'dfn2b_s39b'),\n",
       " ('ViT-L-14-336', 'openai'),\n",
       " ('ViT-H-14', 'laion2b_s32b_b79k'),\n",
       " ('ViT-H-14', 'metaclip_fullcc'),\n",
       " ('ViT-H-14', 'metaclip_altogether'),\n",
       " ('ViT-H-14', 'dfn5b'),\n",
       " ('ViT-H-14-378', 'dfn5b'),\n",
       " ('ViT-g-14', 'laion2b_s12b_b42k'),\n",
       " ('ViT-g-14', 'laion2b_s34b_b88k'),\n",
       " ('ViT-bigG-14', 'laion2b_s39b_b160k'),\n",
       " ('ViT-bigG-14', 'metaclip_fullcc'),\n",
       " ('roberta-ViT-B-32', 'laion2b_s12b_b32k'),\n",
       " ('xlm-roberta-base-ViT-B-32', 'laion5b_s13b_b90k'),\n",
       " ('xlm-roberta-large-ViT-H-14', 'frozen_laion5b_s13b_b90k'),\n",
       " ('convnext_base', 'laion400m_s13b_b51k'),\n",
       " ('convnext_base_w', 'laion2b_s13b_b82k'),\n",
       " ('convnext_base_w', 'laion2b_s13b_b82k_augreg'),\n",
       " ('convnext_base_w', 'laion_aesthetic_s13b_b82k'),\n",
       " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k'),\n",
       " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k_augreg'),\n",
       " ('convnext_large_d', 'laion2b_s26b_b102k_augreg'),\n",
       " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft'),\n",
       " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft_soup'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_rewind'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_soup'),\n",
       " ('coca_ViT-B-32', 'laion2b_s13b_b90k'),\n",
       " ('coca_ViT-B-32', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
       " ('coca_ViT-L-14', 'laion2b_s13b_b90k'),\n",
       " ('coca_ViT-L-14', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
       " ('EVA01-g-14', 'laion400m_s11b_b41k'),\n",
       " ('EVA01-g-14-plus', 'merged2b_s11b_b114k'),\n",
       " ('EVA02-B-16', 'merged2b_s8b_b131k'),\n",
       " ('EVA02-L-14', 'merged2b_s4b_b131k'),\n",
       " ('EVA02-L-14-336', 'merged2b_s6b_b61k'),\n",
       " ('EVA02-E-14', 'laion2b_s4b_b115k'),\n",
       " ('EVA02-E-14-plus', 'laion2b_s9b_b144k'),\n",
       " ('ViT-B-16-SigLIP', 'webli'),\n",
       " ('ViT-B-16-SigLIP-256', 'webli'),\n",
       " ('ViT-B-16-SigLIP-i18n-256', 'webli'),\n",
       " ('ViT-B-16-SigLIP-384', 'webli'),\n",
       " ('ViT-B-16-SigLIP-512', 'webli'),\n",
       " ('ViT-L-16-SigLIP-256', 'webli'),\n",
       " ('ViT-L-16-SigLIP-384', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP', 'webli'),\n",
       " ('ViT-SO400M-16-SigLIP-i18n-256', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP-378', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP-384', 'webli'),\n",
       " ('ViT-B-32-SigLIP2-256', 'webli'),\n",
       " ('ViT-B-16-SigLIP2', 'webli'),\n",
       " ('ViT-B-16-SigLIP2-256', 'webli'),\n",
       " ('ViT-B-16-SigLIP2-384', 'webli'),\n",
       " ('ViT-B-16-SigLIP2-512', 'webli'),\n",
       " ('ViT-L-16-SigLIP2-256', 'webli'),\n",
       " ('ViT-L-16-SigLIP2-384', 'webli'),\n",
       " ('ViT-L-16-SigLIP2-512', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP2', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP2-378', 'webli'),\n",
       " ('ViT-SO400M-16-SigLIP2-256', 'webli'),\n",
       " ('ViT-SO400M-16-SigLIP2-384', 'webli'),\n",
       " ('ViT-SO400M-16-SigLIP2-512', 'webli'),\n",
       " ('ViT-gopt-16-SigLIP2-256', 'webli'),\n",
       " ('ViT-gopt-16-SigLIP2-384', 'webli'),\n",
       " ('ViT-L-14-CLIPA', 'datacomp1b'),\n",
       " ('ViT-L-14-CLIPA-336', 'datacomp1b'),\n",
       " ('ViT-H-14-CLIPA', 'datacomp1b'),\n",
       " ('ViT-H-14-CLIPA-336', 'laion2b'),\n",
       " ('ViT-H-14-CLIPA-336', 'datacomp1b'),\n",
       " ('ViT-bigG-14-CLIPA', 'datacomp1b'),\n",
       " ('ViT-bigG-14-CLIPA-336', 'datacomp1b'),\n",
       " ('nllb-clip-base', 'v1'),\n",
       " ('nllb-clip-large', 'v1'),\n",
       " ('nllb-clip-base-siglip', 'v1'),\n",
       " ('nllb-clip-base-siglip', 'mrl'),\n",
       " ('nllb-clip-large-siglip', 'v1'),\n",
       " ('nllb-clip-large-siglip', 'mrl'),\n",
       " ('MobileCLIP-S1', 'datacompdr'),\n",
       " ('MobileCLIP-S2', 'datacompdr'),\n",
       " ('MobileCLIP-B', 'datacompdr'),\n",
       " ('MobileCLIP-B', 'datacompdr_lt'),\n",
       " ('ViTamin-S', 'datacomp1b'),\n",
       " ('ViTamin-S-LTT', 'datacomp1b'),\n",
       " ('ViTamin-B', 'datacomp1b'),\n",
       " ('ViTamin-B-LTT', 'datacomp1b'),\n",
       " ('ViTamin-L', 'datacomp1b'),\n",
       " ('ViTamin-L-256', 'datacomp1b'),\n",
       " ('ViTamin-L-336', 'datacomp1b'),\n",
       " ('ViTamin-L-384', 'datacomp1b'),\n",
       " ('ViTamin-L2', 'datacomp1b'),\n",
       " ('ViTamin-L2-256', 'datacomp1b'),\n",
       " ('ViTamin-L2-336', 'datacomp1b'),\n",
       " ('ViTamin-L2-384', 'datacomp1b'),\n",
       " ('ViTamin-XL-256', 'datacomp1b'),\n",
       " ('ViTamin-XL-336', 'datacomp1b'),\n",
       " ('ViTamin-XL-384', 'datacomp1b'),\n",
       " ('RN50-quickgelu', 'openai'),\n",
       " ('RN50-quickgelu', 'yfcc15m'),\n",
       " ('RN50-quickgelu', 'cc12m'),\n",
       " ('RN101-quickgelu', 'openai'),\n",
       " ('RN101-quickgelu', 'yfcc15m'),\n",
       " ('RN50x4-quickgelu', 'openai'),\n",
       " ('RN50x16-quickgelu', 'openai'),\n",
       " ('RN50x64-quickgelu', 'openai'),\n",
       " ('ViT-B-32-quickgelu', 'openai'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e31'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e32'),\n",
       " ('ViT-B-32-quickgelu', 'metaclip_400m'),\n",
       " ('ViT-B-32-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-B-16-quickgelu', 'openai'),\n",
       " ('ViT-B-16-quickgelu', 'dfn2b'),\n",
       " ('ViT-B-16-quickgelu', 'metaclip_400m'),\n",
       " ('ViT-B-16-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-L-14-quickgelu', 'openai'),\n",
       " ('ViT-L-14-quickgelu', 'metaclip_400m'),\n",
       " ('ViT-L-14-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-L-14-quickgelu', 'dfn2b'),\n",
       " ('ViT-L-14-336-quickgelu', 'openai'),\n",
       " ('ViT-H-14-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-H-14-quickgelu', 'dfn5b'),\n",
       " ('ViT-H-14-378-quickgelu', 'dfn5b'),\n",
       " ('ViT-bigG-14-quickgelu', 'metaclip_fullcc')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bf5522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATES_BARE = [\"{}\"]\n",
    "TEMPLATES_BARE_PER = [\"{}.\"]\n",
    "TEMPLATES_PREP = [\"a photo of a {}\"]\n",
    "TEMPLATES_PREP_PER = [\"a photo of a {}.\"]\n",
    "TEMPLATES_COMBO = [\"{}\", \"{}.\", \"a photo of a {}\", \"a photo of a {}.\"]\n",
    "\n",
    "TEMPS_EXP1      = [TEMPLATES_BARE, TEMPLATES_BARE_PER, TEMPLATES_PREP, TEMPLATES_PREP_PER,    TEMPLATES_COMBO]\n",
    "TEMP_NAMES_EXP1 = [\"Bare\",         \"Bare w/ period\",   \"Prepending\",   \"Prededing w/ period\", \"Combo\"]\n",
    "\n",
    "####\n",
    "\n",
    "TEMPLATES_CUSTOM = [\n",
    "    \"a lucky {}\",\n",
    "    \"a bad {}\",\n",
    "    \"a good {}\",\n",
    "    \"an excellent {}\",\n",
    "    \"quite a poor {}\",\n",
    "    \"what's going on with this {}\",\n",
    "    \"a {} in the sun\",\n",
    "    \"a {} in the shadows\",\n",
    "    \"a big {}\",\n",
    "    \"a little {}\",\n",
    "    \"a top notch {}\",\n",
    "    \"kind of looks like a {}\",\n",
    "    \"it might be a {}\",\n",
    "    \"one heck of a {}\",\n",
    "    \"the best {} around\",\n",
    "    \"found a {}\",\n",
    "    \"look at this {}\",\n",
    "    \"a great {}\",\n",
    "    \"today I went to the grocery store and I saw a {} along the way there\",\n",
    "    \"yesterday I climbed a mountain and there was a {} at the gas station on the way there\",\n",
    "    \"today in school I learned about the ancient version of a {}\",\n",
    "    \"today on the internet I learned about the {} factory\",\n",
    "    \"what kind of {} is this?\",\n",
    "    \"quite the sharp {}\",\n",
    "    \"quite close to the {}\",\n",
    "    \"the {} is in sight\",\n",
    "    \"today we ran into a {}\",\n",
    "    \"futuristic {}\",\n",
    "    \"this {} will have the last laugh\",\n",
    "    \"what a wild {}\",\n",
    "    \"an untame and unruly {}\",\n",
    "    \"a quivering {}\",\n",
    "    \"a well placed {}\",\n",
    "    \"a green, mean, {} cleaning machine\",\n",
    "    \"a wild {} has appeared!\",\n",
    "    \"{} radio\",\n",
    "    \"the official {} convention is in town\",\n",
    "    \"an amazing photo of a {}\",\n",
    "    \"a decent photo of a {}\",\n",
    "    \"a poor photo of a {}\",\n",
    "    \"a {} at the bottom\",\n",
    "    \"a {} at the top\",\n",
    "    \"this {} is winning\",\n",
    "    \"{} lol\",\n",
    "]\n",
    "TEMPLATES_CLIP80 = IMAGENET_TEMPLATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c069e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "CLIP ResNet-50 (224px)\n",
      "\n",
      "--------------------------------------------\n",
      "Custom + CLIP 80\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating text embeddings: 100%|██████████| 1000/1000 [00:27<00:00, 36.91it/s]\n",
      "Evaluation: 100%|██████████| 25/25 [00:37<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prec@1: 59.9%\n",
      "\n",
      "============================================\n",
      "CLIP ViT-B/32 (224px)\n",
      "\n",
      "--------------------------------------------\n",
      "Custom + CLIP 80\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating text embeddings: 100%|██████████| 1000/1000 [00:25<00:00, 38.76it/s]\n",
      "Evaluation: 100%|██████████| 25/25 [00:34<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prec@1: 63.3%\n",
      "\n",
      "============================================\n",
      "CLIP ViT-B/16 (224px)\n",
      "\n",
      "--------------------------------------------\n",
      "Custom + CLIP 80\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating text embeddings: 100%|██████████| 1000/1000 [00:25<00:00, 39.84it/s]\n",
      "Evaluation: 100%|██████████| 25/25 [00:47<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prec@1: 68.3%\n",
      "\n",
      "============================================\n",
      "CLIP ViT-L/14 (224px)\n",
      "\n",
      "--------------------------------------------\n",
      "Custom + CLIP 80\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating text embeddings: 100%|██████████| 1000/1000 [00:43<00:00, 22.87it/s]\n",
      "Evaluation: 100%|██████████| 25/25 [02:44<00:00,  6.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prec@1: 75.6%\n",
      "\n",
      "============================================\n",
      "CLIP ViT-L/14 (336px)\n",
      "\n",
      "--------------------------------------------\n",
      "Custom + CLIP 80\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating text embeddings: 100%|██████████| 1000/1000 [00:43<00:00, 22.89it/s]\n",
      "Evaluation: 100%|██████████| 25/25 [06:15<00:00, 15.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prec@1: 76.7%\n",
      "\n",
      "============================================\n",
      "SigLIP ViT-B/16 (224px)\n",
      "\n",
      "--------------------------------------------\n",
      "Custom + CLIP 80\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating text embeddings: 100%|██████████| 1000/1000 [00:30<00:00, 33.16it/s]\n",
      "Evaluation: 100%|██████████| 25/25 [00:48<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prec@1: 76.2%\n",
      "\n",
      "============================================\n",
      "SigLIP ViT-B/16 (256px)\n",
      "\n",
      "--------------------------------------------\n",
      "Custom + CLIP 80\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating text embeddings: 100%|██████████| 1000/1000 [00:29<00:00, 33.81it/s]\n",
      "Evaluation: 100%|██████████| 25/25 [01:00<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prec@1: 76.7%\n",
      "\n",
      "============================================\n",
      "SigLIP ViT-L/16 (256px)\n",
      "\n",
      "--------------------------------------------\n",
      "Custom + CLIP 80\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating text embeddings: 100%|██████████| 1000/1000 [01:34<00:00, 10.56it/s]\n",
      "Evaluation: 100%|██████████| 25/25 [02:44<00:00,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prec@1: 80.7%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEMP_GROUP = TEMPS_EXP1\n",
    "# TG_NAMES   = TEMP_NAMES_EXP1\n",
    "\n",
    "# TEMP_GROUPS = [TEMPLATES_CLIP80]\n",
    "# TG_NAMES   = [\"CLIP 80\"]\n",
    "\n",
    "# TEMP_GROUPS = [TEMPLATES_CUSTOM]\n",
    "# TG_NAMES    = [\"Custom\"]\n",
    "\n",
    "TEMP_GROUPS = [TEMPLATES_CUSTOM + TEMPLATES_CLIP80]\n",
    "TG_NAMES    = [\"Custom + CLIP 80\"]\n",
    "\n",
    "def run_experiment(template_groups, tg_names):\n",
    "\n",
    "    VLM_CONFIGS = [\n",
    "        (\"RN50\",                \"openai\", True,  \"CLIP ResNet-50 (224px)\"),\n",
    "        (\"ViT-B-32\",            \"openai\", True,  \"CLIP ViT-B/32 (224px)\"),\n",
    "        (\"ViT-B-16\",            \"openai\", True,  \"CLIP ViT-B/16 (224px)\"),\n",
    "        (\"ViT-L-14\",            \"openai\", True,  \"CLIP ViT-L/14 (224px)\"),\n",
    "        (\"ViT-L-14-336\",        \"openai\", True,  \"CLIP ViT-L/14 (336px)\"),\n",
    "        (\"ViT-B-16-SigLIP\",     \"webli\",  False, \"SigLIP ViT-B/16 (224px)\"),\n",
    "        (\"ViT-B-16-SigLIP-256\", \"webli\",  False, \"SigLIP ViT-B/16 (256px)\"),\n",
    "        (\"ViT-L-16-SigLIP-256\", \"webli\",  False, \"SigLIP ViT-L/16 (256px)\"),\n",
    "    ]\n",
    "\n",
    "    for model_id, pretrained, quick_gelu, model_name in VLM_CONFIGS:\n",
    "\n",
    "\n",
    "        print(\n",
    "            f\"============================================\",\n",
    "            f\"{model_name}\",\n",
    "            f\"\",\n",
    "            sep=\"\\n\"\n",
    "        )\n",
    "\n",
    "        for templates, exp_name in zip(template_groups, tg_names):\n",
    "            print(\n",
    "                f\"--------------------------------------------\",\n",
    "                f\"{exp_name}\",\n",
    "                f\"\",\n",
    "                sep=\"\\n\"\n",
    "            )\n",
    "\n",
    "            \n",
    "\n",
    "            model, _, img_pp = open_clip.create_model_and_transforms(model_id, pretrained=pretrained, quick_gelu=quick_gelu, device=device)\n",
    "            model.eval()\n",
    "            tokenizer = open_clip.get_tokenizer(model_id)\n",
    "\n",
    "            dataloader     = spawn_dataloader(dpath_valid, img_pp, BATCH_SIZE, N_WORKERS)\n",
    "            class_embs_txt = build_class_protos_text(model, tokenizer, IMAGENET_CLASSES, templates, device).to(device)\n",
    "\n",
    "            run_inference(model, dataloader, device, model_name, class_embs_txt=class_embs_txt)\n",
    "\n",
    "\n",
    "run_experiment(TEMP_GROUPS, TG_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e5dcd",
   "metadata": {},
   "source": [
    "Note: `RN50` in the printout refers to CLIP ResNet-50 and `ResNet` refers to the standalone ResNet-50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc890fa",
   "metadata": {},
   "source": [
    "Maybe perform an ablation of the text ensemble: just encode the labels straight up and compare to the templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b67ae93",
   "metadata": {},
   "source": [
    "Not the 76.2% reported in the seminal CLIP paper (Table 11), but close enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2736042",
   "metadata": {},
   "source": [
    "We conclude with a discussion of the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95d2b4e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
