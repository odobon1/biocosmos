logits:
  scale_init:   null  # null for existing learnable param (or 0.0 non-learnable if non-existent), float for learnable param override init
  bias_init:    null  # null for existing learnable param (or 0.0 non-learnable if non-existent), float for learnable param override init
  freeze_scale: false  # (true) non-learnable temp
  freeze_bias:  false  # (true) non-learnable bias

class_weighting:
  type:     inv_freq  # options: {inv_freq, class_balanced}
  if_gamma: 0.5  # (0.0, 1.0]; "inverse-frequency gamma", only used for type: inv_freq; 1 / n_c^(if_gamma); if_gamma = 1.0 --> 1 / n_c (classic scheme), if_gamma = 0.5 --> 1 / sqrt(n_c), if_gamma = 0.0 --> no weighting (uniform)
  cb_beta:  0.9999  # [0.0, 1.0); only used for type: class_balanced; controls aggressiveness
  cp_type:  1  # options: {1, 2}; class-pair computation type; 1: vanilla (i2t, t2i for same pair treated as separate "classes"), 2: negative pairs multiplied by 2, upper triangle extracted and used to normalize, full matrix constructed with normalized weights (i2t, t2i for same pair treated as same "classes")

focal:
  gamma:     2.0  # [0, inf); down-weights easy samples, larger gamma --> stronger focus on hard cases; 1-2 for softmax (InfoNCE), 2 for sigmoid (BCE); FL reduces to CE when gamma = 0
  comp_type: 1  # options: {1, 2}; continuous-form focal loss computation type; identical behavior to discrete form when targets=0/1; not relevant for 1D-weighted InfoNCE

# only relevant for BCE (and maybe regression)
dyn_posneg: true  # dynamic reweighting of pos/neg; intended for use with multi-pos, also convenient for aligned, not compatible with hierarchical

regression:
  huber_beta: 0.1