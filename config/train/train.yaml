study_name:      dev
experiment_name: dev
seed:            42
# split_name:      S29-42
split_name:      P38-42
# split_name:      dev

verbose_batch_loss: false

# whether to allow trials to overwrite data at the study/experiment/trial level (for dev/debug, should otherwise be set to false)
allow_overwrite_trial: true
allow_diff_study:      true
allow_diff_experiment: true

model_type: clip_vitb16  # common options: {bioclip, clip_vitb16, siglip_vitb16}
non_causal: false  # (true) converts text encoder causal attention mask to non-causal (zeros on diagonal + lower triangle, -inf on upper --> all zeros) -- only applicable to CLIP and ViTamin (SigLIP already uses non-causal i.e. bidirectional)

loss_type:   infonce1  # options: {infonce1, infonce2, sigmoid, mse, huber}
targ_type:   phylo  # options: {aligned, multipos, hierarchical, phylo}
class_wting: false
focal:       false

n_epochs:    200
chkpt_every: 1
# batch_size:  32
# batch_size:  512
batch_size:  1024
# batch_size:  2048
# batch_size:  4096
# batch_size:  8192
# batch_size:  16384
# batch_size:  32768

drop_partial_batch_train: true

lr_init:       1.0e-5
lr_sched_type: exp
weight_decay:  0.0  # CLIP paper recommends weight_decay = 0.2 <-- this may be just for pretrain, look into it
beta1:         0.9  # CLIP paper recommends Beta1 = 0.9
beta2:         0.95  # seminal papers recommend Beta2 = 0.999 (CLIP ResNet) / 0.98 (CLIP ViT) / 0.95 (SigLIP) ~ careful, may be for pretraining not fine-tuning
eps:           1.0e-6  # CLIP paper recommends eps = 1.0e-8 (ResNet) / 1.0e-6 (ViT)

freeze_text_encoder:  false
freeze_image_encoder: false

cached_imgs: null  # options: {null, pl, pp}
mixed_prec:  true  # mixed precision for training and validation
act_chkpt:   true  # activation checkpointing

# text_preps_type_train: mixed
text_preps_type_train: combo_temps
text_preps_type_val:   bioclip_sci
